import argparse
from datasets import load_dataset

from models.gpt2_finetuned_model import FinetuneGPT2
from transformers import Trainer, TrainingArguments

def train(args):
    device = args.device
    batch_size = args.batch_size

    gpt_model = FinetuneGPT2(args)
    gpt_model.build_model(checkpoint_dir=args.checkpoint)

    dataset = load_dataset('text', data_files=[args.data_path]).train_test_split(test_size=0.1)

    print("Start training")
    last_step = 0
    for begin_loc in range(0, len(dataset['train']), batch_size):
        last_step += 1

    training_args = TrainingArguments(
        output_dir=args.save_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.eval_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        warmup_steps=300,  # warmup_steps=gpt_model.num_warmup_steps,
        weight_decay=args.weight_decay,
        evaluate_during_training=True,
        save_steps=args.save_steps,
        eval_steps=args.save_steps,
        seed=args.seed
    )

    trainer = Trainer(
        model=gpt_model.model,
        args=training_args,
        train_dataset=dataset['train'],
        eval_dataset=dataset['test'],
        prediction_loss_only=True
    )

    trainer.train()
    trainer.save_model()

    trainer.evaluate()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Finetune GPT-2 model on corrupted corpus")

    parser.add_argument("-data_path", "--data_path", type=str, help="Path to the training sequences generated by corrput_corpus.py")
    parser.add_argument('--device', type=str, default='cuda', help='{cuda, cpu}')
    parser.add_argument('--checkpoint', type=str, default=None, help='Path to LOAD model checkpoint')
    parser.add_argument('--model', type=str, default='gpt2-medium', help='pretrained model name (only gpt available)')

    # Training arguments
    parser.add_argument('--save_dir', type=str, help='Path to SAVE model checkpoint')
    parser.add_argument('-ep', '--epochs', type=int, default=5, help='Number of epochs')
    parser.add_argument('-bs', '--batch_size', type=int, default=2, help='Training batch size')
    parser.add_argument('--eval_batch_size', type=int, default=4, help='Evaluation batch size')
    parser.add_argument('--gradient_accumulation', type=int, default=1, help='Number of update steps to accumulate the gradients')
    parser.add_argument('-lr', '--learning_rate', type=float, default=6.25e-5, help='Learning rate of fine-tuning')
    parser.add_argument('--weight_decay', type=float, default=0.01)
    parser.add_argument('--save_steps', type=int, default=1000, help='Number of update steps before eval & save')

    parser.add_argument('--seed', type=int, default=None)

    args = parser.parse_args()



    train(args)